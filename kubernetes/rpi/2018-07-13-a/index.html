<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Deploy Helm and Tiller on Rasberry PI Cluster - Jerome Brette&#39;s Blog</title>
<meta name="description" content="Jerome Brette&#39;s Blog">
<meta name="generator" content="Hugo 0.92.2" />
<link href="https://jbrette.github.io//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="https://jbrette.github.io/kubernetes/rpi/2018-07-13-a/">
<link rel="stylesheet" href="https://jbrette.github.io/css/theme.min.css">
<link rel="stylesheet" href="https://jbrette.github.io/css/chroma.min.css">
<script defer src="https://jbrette.github.io//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="https://jbrette.github.io/js/bundle.js"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123357787-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-123357787-1');
</script>
<style>
:root {}
</style>
<meta property="og:title" content="Deploy Helm and Tiller on Rasberry PI Cluster" />
<meta property="og:description" content="Goal The main purpose of this exercise is to be able to use Helm on the Rapsberry PI Cluster.
Key Aspects  The goal is to setup helm and tiller on the Raspberry PI cluster Having the golang, glide&hellip;and related libraries setup in a PI for compilation is kind of complicated. I started but encounter too many issues (even small), had to install too many compilation related packages on my PI system, hence decided to use an Ubuntu VM to compile and prepare the binaries for image for helm and tiller." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jbrette.github.io/kubernetes/rpi/2018-07-13-a/" /><meta property="og:image" content="https://jbrette.github.io/images/og-image.png"/><meta property="article:section" content="kubernetes" />
<meta property="article:published_time" content="2018-07-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-07-13T00:00:00+00:00" /><meta property="og:site_name" content="Hugo Techdoc Theme" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://jbrette.github.io/images/og-image.png"/>

<meta name="twitter:title" content="Deploy Helm and Tiller on Rasberry PI Cluster"/>
<meta name="twitter:description" content="Goal The main purpose of this exercise is to be able to use Helm on the Rapsberry PI Cluster.
Key Aspects  The goal is to setup helm and tiller on the Raspberry PI cluster Having the golang, glide&hellip;and related libraries setup in a PI for compilation is kind of complicated. I started but encounter too many issues (even small), had to install too many compilation related packages on my PI system, hence decided to use an Ubuntu VM to compile and prepare the binaries for image for helm and tiller."/>
<meta itemprop="name" content="Deploy Helm and Tiller on Rasberry PI Cluster">
<meta itemprop="description" content="Goal The main purpose of this exercise is to be able to use Helm on the Rapsberry PI Cluster.
Key Aspects  The goal is to setup helm and tiller on the Raspberry PI cluster Having the golang, glide&hellip;and related libraries setup in a PI for compilation is kind of complicated. I started but encounter too many issues (even small), had to install too many compilation related packages on my PI system, hence decided to use an Ubuntu VM to compile and prepare the binaries for image for helm and tiller."><meta itemprop="datePublished" content="2018-07-13T00:00:00+00:00" />
<meta itemprop="dateModified" content="2018-07-13T00:00:00+00:00" />
<meta itemprop="wordCount" content="1887"><meta itemprop="image" content="https://jbrette.github.io/images/og-image.png"/>
<meta itemprop="keywords" content="kubedge,kubernetes,helm," /></head>
<body><div class="container"><header>
<h1>Jerome Brette&#39;s Blog</h1>
<p class="description">Jerome Brette&#39;s Blog</p>

</header>
<div class="global-menu">
<nav>
<ul>
<li id="home" class=""><a href="/"><i class='fa fa-heart'></i>&nbsp;Home</a></li>
<li id="medium" class="twitter-menu-item"><a href="https://medium.com/@jbrette">Medium</a></li>
<li id="linkedin" class="twitter-menu-item"><a href="https://www.linkedin.com/in/jerome-brette-109b8711/">LinkedIn</a></li>
<li class=""><a href="/about/">About</a></li><li class="parent "><a href="">Tutorials<i class="fas fa-angle-right"></i></a>
<ul class="sub-menu">
<li class="child "><a href="/post/2018-09-26-a/">Blinkt</a></li>
</ul>
</li></ul>
</nav>
</div>

<div class="content-container">
<main><h1>Deploy Helm and Tiller on Rasberry PI Cluster</h1>
<h1 id="goal">Goal</h1>
<p>The main purpose of this exercise is to be able to use Helm on the Rapsberry PI Cluster.</p>
<h2 id="key-aspects">Key Aspects</h2>
<ul>
<li>The goal is to setup helm and tiller on the Raspberry PI cluster</li>
<li>Having the golang, glide&hellip;and related libraries setup in a PI for compilation
is kind of complicated. I started but encounter too many issues (even small), had
to install too many compilation related packages on my PI system, hence decided to
use an Ubuntu VM to compile and prepare the binaries for image for helm and tiller.
This will be usefull to setup CI/CD pipeline with Travis-CI.</li>
</ul>
<h2 id="build-tiller-executable-and-docker-image">Build Tiller executable and Docker image</h2>
<h3 id="cross-compiling-from-ubuntu-machine">Cross Compiling from Ubuntu Machine</h3>
<p>First check the go setup. Fetch the code</p>
<p>Let&rsquo;s check the go environment. See <a href="https://jbrette.github.io/jekyll/update/2018/06/22/a.html">Setup GOLANG environment</a></p>
<pre tabindex="0"><code>which go

/usr/bin/go
</code></pre><pre tabindex="0"><code>go version

go version go1.10.1 linux/amd64
</code></pre><pre tabindex="0"><code>export GOPATH=$HOME/src
mkdir -p src/k8s.io
cd src/k8s.io

git clone -b release-2.9 git@github.com:kubernetes/helm.git
</code></pre><pre tabindex="0"><code>cd $HOME/src/k8s.io/helm
make clean bootstrap build-cross dist APP=helm VERSION=2.9 TARGETS=linux/arm

...

go build -o bin/protoc-gen-go ./vendor/github.com/golang/protobuf/protoc-gen-go
CGO_ENABLED=0 gox -parallel=3 -output=&quot;_dist/{{.OS}}-{{.Arch}}/{{.Dir}}&quot; -osarch='linux/arm'  -tags '' -ldflags '-w -s -X k8s.io/helm/pkg/version.Version=2.9 -X k8s.io/helm/pkg/version.BuildMetadata= -X k8s.io/helm/pkg/version.GitCommit=20adb27c7c5868466912eebdf6664e7390ebe710 -X k8s.io/helm/pkg/version.GitTreeState=clean -extldflags &quot;-static&quot;' k8s.io/helm/cmd/helm
Number of parallel builds: 3

--&gt;       linux/arm: k8s.io/helm/cmd/helm
( \
        cd _dist &amp;&amp; \
        find * -type d -exec cp ../LICENSE {} \; &amp;&amp; \
        find * -type d -exec cp ../README.md {} \; &amp;&amp; \
        find * -type d -exec tar -zcf helm-2.9-{}.tar.gz {} \; &amp;&amp; \
        find * -type d -exec zip -r helm-2.9-{}.zip {} \; \
)
  adding: linux-arm/ (stored 0%)
  adding: linux-arm/LICENSE (deflated 65%)
  adding: linux-arm/README.md (deflated 59%)
  adding: linux-arm/helm (deflated 67%)
</code></pre><p>Let&rsquo;s transfer the helm binarie to RPI</p>
<pre tabindex="0"><code>scp -r _dist/linux-arm/ rpiuser@192.168.1.95:/home/rpiuser/helm_binaries
</code></pre><h3 id="cross-compiling-and-image-cration-from-travis-ci">Cross Compiling and Image cration from Travis-CI</h3>
<ul>
<li>WIP</li>
</ul>
<h2 id="helm">Helm</h2>
<h3 id="cross-compiling-from-ubuntu-machine-1">Cross Compiling from Ubuntu Machine</h3>
<p>Let&rsquo;s assume we cloned the code for helm already.</p>
<pre tabindex="0"><code>make clean bootstrap build-cross dist APP=tiller VERSION=2.9 TARGETS=linux/arm

...
go build -o bin/protoc-gen-go ./vendor/github.com/golang/protobuf/protoc-gen-go
CGO_ENABLED=0 gox -parallel=3 -output=&quot;_dist/{{.OS}}-{{.Arch}}/{{.Dir}}&quot; -osarch='linux/arm'  -tags '' -ldflags '-w -s -X k8s.io/helm/pkg/version.Version=2.9 -X k8s.io/helm/pkg/version.BuildMetadata= -X k8s.io/helm/pkg/version.GitCommit=20adb27c7c5868466912eebdf6664e7390ebe710 -X k8s.io/helm/pkg/version.GitTreeState=clean -extldflags &quot;-static&quot;' k8s.io/helm/cmd/tiller
Number of parallel builds: 3

--&gt;       linux/arm: k8s.io/helm/cmd/tiller
( \
        cd _dist &amp;&amp; \
        find * -type d -exec cp ../LICENSE {} \; &amp;&amp; \
        find * -type d -exec cp ../README.md {} \; &amp;&amp; \
        find * -type d -exec tar -zcf helm-2.9-{}.tar.gz {} \; &amp;&amp; \
        find * -type d -exec zip -r helm-2.9-{}.zip {} \; \
)
  adding: linux-arm/ (stored 0%)
  adding: linux-arm/LICENSE (deflated 65%)
  adding: linux-arm/README.md (deflated 59%)
  adding: linux-arm/tiller (deflated 67%)
</code></pre><h3 id="build-the-docker-image-on-the-remote-pi">Build the docker image on the remote PI</h3>
<p>Create a Dockerfile called rootfs/Dockerfile.arm32v7</p>
<pre tabindex="0"><code>FROM debian:jessie-slim

RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends ca-certificates \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

ENV HOME /tmp

COPY _dist/linux-arm /

EXPOSE 44134

CMD [&quot;/tiller&quot;]
</code></pre><pre tabindex="0"><code>export DHUBREPO=jbrette/tiller-arm32v7
export VERSION=2.9
docker build -t $DHUBREPO:$VERSION -f rootfs/Dockerfile.arm32v7 .

Sending build context to Docker daemon  284.1MB
Step 1/6 : FROM debian:jessie-slim
 ---&gt; d273fca45b31
Step 2/6 : RUN apt-get update     &amp;&amp; apt-get install -y --no-install-recommends ca-certificates     &amp;&amp; rm -rf /var/lib/apt/lists/*
 ---&gt; Running in e51ea5c31ad7
Get:1 http://security.debian.org jessie/updates InRelease [44.9 kB]
Ign http://deb.debian.org jessie InRelease
Get:2 http://deb.debian.org jessie-updates InRelease [145 kB]
Get:3 http://deb.debian.org jessie Release.gpg [2420 B]
Get:4 http://deb.debian.org jessie Release [148 kB]
Get:5 http://security.debian.org jessie/updates/main armel Packages [576 kB]
Get:6 http://deb.debian.org jessie-updates/main armel Packages [23.7 kB]
Get:7 http://deb.debian.org jessie/main armel Packages [8902 kB]
Fetched 9843 kB in 49s (199 kB/s)
Reading package lists...
Reading package lists...
Building dependency tree...
Reading state information...
The following extra packages will be installed:
  libssl1.0.0 openssl
The following NEW packages will be installed:
  ca-certificates libssl1.0.0 openssl
0 upgraded, 3 newly installed, 0 to remove and 1 not upgraded.
Need to get 1692 kB of archives.
After this operation, 3770 kB of additional disk space will be used.
Get:1 http://security.debian.org/debian-security/ jessie/updates/main ca-certificates all 20141019+deb8u4 [185 kB]
Get:2 http://deb.debian.org/debian/ jessie/main libssl1.0.0 armel 1.0.1t-1+deb8u8 [852 kB]
Get:3 http://deb.debian.org/debian/ jessie/main openssl armel 1.0.1t-1+deb8u8 [655 kB]
debconf: delaying package configuration, since apt-utils is not installed
Fetched 1692 kB in 7s (235 kB/s)
Selecting previously unselected package libssl1.0.0:armel.
(Reading database ... 7451 files and directories currently installed.)
Preparing to unpack .../libssl1.0.0_1.0.1t-1+deb8u8_armel.deb ...
Unpacking libssl1.0.0:armel (1.0.1t-1+deb8u8) ...
Selecting previously unselected package openssl.
Preparing to unpack .../openssl_1.0.1t-1+deb8u8_armel.deb ...
Unpacking openssl (1.0.1t-1+deb8u8) ...
Selecting previously unselected package ca-certificates.
Preparing to unpack .../ca-certificates_20141019+deb8u4_all.deb ...
Unpacking ca-certificates (20141019+deb8u4) ...
Setting up libssl1.0.0:armel (1.0.1t-1+deb8u8) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/arm-linux-gnueabi/perl/5.20.2 /usr/local/share/perl/5.20.2 /usr/lib/arm-linux-gnueabi/perl5/5.20 /usr/share/perl5 /usr/lib/arm-linux-gnueabi/perl/5.20 /usr/share/perl/5.20 /usr/local/lib/site_perl .) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)
debconf: falling back to frontend: Teletype
Setting up openssl (1.0.1t-1+deb8u8) ...
Setting up ca-certificates (20141019+deb8u4) ...
debconf: unable to initialize frontend: Dialog
debconf: (TERM is not set, so the dialog frontend is not usable.)
debconf: falling back to frontend: Readline
debconf: unable to initialize frontend: Readline
debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/arm-linux-gnueabi/perl/5.20.2 /usr/local/share/perl/5.20.2 /usr/lib/arm-linux-gnueabi/perl5/5.20 /usr/share/perl5 /usr/lib/arm-linux-gnueabi/perl/5.20 /usr/share/perl/5.20 /usr/local/lib/site_perl .) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.)
debconf: falling back to frontend: Teletype
Updating certificates in /etc/ssl/certs... 152 added, 0 removed; done.
Processing triggers for libc-bin (2.19-18+deb8u10) ...
Processing triggers for ca-certificates (20141019+deb8u4) ...
Updating certificates in /etc/ssl/certs... 0 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....done.
Removing intermediate container e51ea5c31ad7
 ---&gt; 1dccf46769e7
Step 3/6 : ENV HOME /tmp
 ---&gt; Running in 45de8c047bd3
Removing intermediate container 45de8c047bd3
 ---&gt; baa6d8b15164
Step 4/6 : COPY _dist/linux-arm /
 ---&gt; 59a6cc8bfdbe
Step 5/6 : EXPOSE 44134
 ---&gt; Running in d2c6d9b3a3bc
Removing intermediate container d2c6d9b3a3bc
 ---&gt; 28a331217c52
Step 6/6 : CMD [&quot;/tiller&quot;]
 ---&gt; Running in 9760ea4dc6c5
Removing intermediate container 9760ea4dc6c5
 ---&gt; 9384b4f39ab3
Successfully built 9384b4f39ab3
Successfully tagged jbrette/tiller-arm32v7:2.9
</code></pre><h3 id="cross-compiling-and-image-cration-from-travis-ci-1">Cross Compiling and Image cration from Travis-CI</h3>
<ul>
<li>WIP</li>
</ul>
<h2 id="install-tiller-and-helm-on-rpi">Install Tiller and Helm on RPI</h2>
<h3 id="install-helm-on-pi">Install helm on PI</h3>
<pre tabindex="0"><code>ssh rpiuser@192.168.1.95
sudo cp /home/rpiuser/helm_binaries/helm /usr/bin/helm
</code></pre><h3 id="deploy-tiller-pod">Deploy Tiller POD</h3>
<p>First push the tiller docker image to the repo</p>
<pre tabindex="0"><code>docker login

docker push jbrette/tiller-arm32v7:2.9
</code></pre><p>Prepare the POD deployment files</p>
<pre tabindex="0"><code>mkdir -p ~/kube-deployments/tiller
cd  ~/kube-deployments/tiller
</code></pre><p>Create the kubectl file for tiller service account</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">cat tiller-serviceaccount.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
...
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller</span>
    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
...
</code></pre></div><p>Create the tiller service account</p>
<pre tabindex="0"><code>kubectl create -f tiller-serviceaccount.yaml

serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller created
</code></pre><p>Create manually the tiller deployment file and leverage the tiller service account</p>
<pre tabindex="0"><code>helm init --service-account tiller --output yaml &gt; tiller.yaml
</code></pre><p>Edit the image and replace <code>gcr.io/kubernetes-helm/tiller:2.9</code> by <code>jbrette/tiller-arm32v7:2.9</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">helm</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller-deploy</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
  <span style="color:#f92672">strategy</span>: {}
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">helm</span>
        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">env</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">TILLER_NAMESPACE</span>
          <span style="color:#f92672">value</span>: <span style="color:#ae81ff">kube-system</span>
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">TILLER_HISTORY_MAX</span>
          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;0&#34;</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">gcr.io/kubernetes-helm/tiller:2.9</span>
        <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#ae81ff">IfNotPresent</span>
        <span style="color:#f92672">livenessProbe</span>:
          <span style="color:#f92672">httpGet</span>:
            <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/liveness</span>
            <span style="color:#f92672">port</span>: <span style="color:#ae81ff">44135</span>
          <span style="color:#f92672">initialDelaySeconds</span>: <span style="color:#ae81ff">1</span>
          <span style="color:#f92672">timeoutSeconds</span>: <span style="color:#ae81ff">1</span>
        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller</span>
        <span style="color:#f92672">ports</span>:
        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">44134</span>
          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tiller</span>
        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">44135</span>
          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
        <span style="color:#f92672">readinessProbe</span>:
          <span style="color:#f92672">httpGet</span>:
            <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/readiness</span>
            <span style="color:#f92672">port</span>: <span style="color:#ae81ff">44135</span>
          <span style="color:#f92672">initialDelaySeconds</span>: <span style="color:#ae81ff">1</span>
          <span style="color:#f92672">timeoutSeconds</span>: <span style="color:#ae81ff">1</span>
        <span style="color:#f92672">resources</span>: {}
      <span style="color:#f92672">serviceAccountName</span>: <span style="color:#ae81ff">tiller</span>
<span style="color:#f92672">status</span>: {}
</code></pre></div><p>Deploy tiller in the kubernetes cluster</p>
<pre tabindex="0"><code>kubectl create -f tiller.yaml
</code></pre><p>Check the deployment state</p>
<pre tabindex="0"><code>kubectl get all -n kube-system

NAME                                        READY     STATUS             RESTARTS   AGE
pod/tiller-deploy-b59fcc885-66l7s           1/1       Running            0          5m

NAME                                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/tiller-deploy          1         1         1            1           5m

NAME                                              DESIRED   CURRENT   READY     AGE
replicaset.apps/tiller-deploy-b59fcc885           1         1         1         5m
</code></pre><p>Check the logs</p>
<pre tabindex="0"><code>kubectl logs pod/tiller-deploy-b59fcc885-66l7s -n kube-system

[main] 2018/07/15 18:08:10 Starting Tiller 2.9 (tls=false)
[main] 2018/07/15 18:08:10 GRPC listening on :44134
[main] 2018/07/15 18:08:10 Probes listening on :44135
[main] 2018/07/15 18:08:10 Storage driver is ConfigMap
[main] 2018/07/15 18:08:10 Max history per release is 0
</code></pre><h3 id="init-the-helm-client-side">Init the Helm client side</h3>
<pre tabindex="0"><code>helm init --client-only

Creating /home/rpiuser/.helm
Creating /home/rpiuser/.helm/repository
Creating /home/rpiuser/.helm/repository/cache
Creating /home/rpiuser/.helm/repository/local
Creating /home/rpiuser/.helm/plugins
Creating /home/rpiuser/.helm/starters
Creating /home/rpiuser/.helm/cache/archive
Creating /home/rpiuser/.helm/repository/repositories.yaml
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com
Adding local repo with URL: http://127.0.0.1:8879/charts
$HELM_HOME has been configured at /home/rpiuser/.helm.
Not installing Tiller due to 'client-only' flag having been set
Happy Helming!
</code></pre><p>Check helm commands are working properly</p>
<pre tabindex="0"><code>helm ls
helm search stable/nginx-ingress
helm inspect stable/nginx-ingress
</code></pre><h2 id="use-helm-on-rpi">Use helm on RPI</h2>
<p>Let&rsquo;s access the simple helm repo designed for</p>
<pre tabindex="0"><code>helm repo add kubeplay 'https://raw.githubusercontent.com/jbrette/kubeplay/arm32v7/helmrepo/'
</code></pre><pre tabindex="0"><code>helm repo list

NAME            URL
stable          https://kubernetes-charts.storage.googleapis.com
local           http://127.0.0.1:8879/charts
kubeplay        https://raw.githubusercontent.com/jbrette/kubeplay/arm32v7/helmrepo/
</code></pre><pre tabindex="0"><code>helm search kubeplay

NAME                            CHART VERSION   APP VERSION     DESCRIPTION
kubeplay/kubeplay-arm32v7       0.1.0           0.1.0           A Helm chart for Kubernetes
</code></pre><p>Install simple hello world helm chart from kubeplay</p>
<pre tabindex="0"><code>helm install kubeplay/kubeplay-arm32v7 --name helm-rpi

NAME:   helm-rpi
LAST DEPLOYED: Sun Jul 15 19:23:58 2018
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&gt; v1/Service
NAME                       TYPE      CLUSTER-IP     EXTERNAL-IP  PORT(S)         AGE
helm-rpi-kubeplay-arm32v7  NodePort  10.96.168.184  &lt;none&gt;       8005:31439/TCP  2s

==&gt; v1beta1/Deployment
NAME                       DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
helm-rpi-kubeplay-arm32v7  1        0        0           0          2s
</code></pre><p>Check the installation</p>
<pre tabindex="0"><code>helm ls

NAME            REVISION        UPDATED                         STATUS          CHART                   NAMESPACE
helm-rpi        1               Sun Jul 15 19:23:58 2018        DEPLOYED        kubeplay-arm32v7-0.1.0  default
</code></pre><p>Check with kubectl if the POD is ready</p>
<pre tabindex="0"><code>kubectl get all -n default

NAME                                             READY     STATUS    RESTARTS   AGE
pod/helm-rpi-kubeplay-arm32v7-58567c4756-fj968   0/1       Running   4          4m

NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/helm-rpi-kubeplay-arm32v7   NodePort    10.96.168.184   &lt;none&gt;        8005:31439/TCP   4m
service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          9d

NAME                                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/helm-rpi-kubeplay-arm32v7   1         1         1            0           4m

NAME                                                   DESIRED   CURRENT   READY     AGE
replicaset.apps/helm-rpi-kubeplay-arm32v7-58567c4756   1         1         0         4m
</code></pre><p>Helm chart is not really consistent. Current image hardcoded to listen on port 80
Let&rsquo;s use helm upgrade to change the value</p>
<pre tabindex="0"><code>helm upgrade --set service.internalPort=80 helm-rpi kubeplay/kubeplay-arm32v7

Release &quot;helm-rpi&quot; has been upgraded. Happy Helming!
LAST DEPLOYED: Sun Jul 15 20:39:17 2018
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==&gt; v1beta1/Deployment
NAME                       DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
helm-rpi-kubeplay-arm32v7  1        1        0           0          1h

==&gt; v1/Pod(related)
NAME                                        READY  STATUS       RESTARTS  AGE
helm-rpi-kubeplay-arm32v7-58567c4756-fj968  0/1    Terminating  25        1h
helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m  0/1    Pending      0         0s

==&gt; v1/Service
NAME                       TYPE      CLUSTER-IP     EXTERNAL-IP  PORT(S)         AGE
helm-rpi-kubeplay-arm32v7  NodePort  10.96.168.184  &lt;none&gt;       8005:31439/TCP  1h
</code></pre><p>The pod seems to now be running properly.</p>
<pre tabindex="0"><code>kubectl get all -n default

NAME                                             READY     STATUS    RESTARTS   AGE
pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m   1/1       Running   0          3m

NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/helm-rpi-kubeplay-arm32v7   NodePort    10.96.168.184   &lt;none&gt;        8005:31439/TCP   1h
service/kubernetes                  ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          9d

NAME                                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/helm-rpi-kubeplay-arm32v7   1         1         1            1           1h

NAME                                                   DESIRED   CURRENT   READY     AGE
replicaset.apps/helm-rpi-kubeplay-arm32v7-58567c4756   0         0         0         1h
replicaset.apps/helm-rpi-kubeplay-arm32v7-6cb66496c6   1         1         1         3m
</code></pre><p>Check the POD</p>
<pre tabindex="0"><code>$ kubectl describe pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m
Name:           helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m
Namespace:      default
Node:           nas-pi/192.168.1.93
Start Time:     Sun, 15 Jul 2018 20:39:18 +0000
Labels:         app=kubeplay-arm32v7
                pod-template-hash=2762205272
                release=helm-rpi
Annotations:    &lt;none&gt;
Status:         Running
IP:             10.244.2.6
Controlled By:  ReplicaSet/helm-rpi-kubeplay-arm32v7-6cb66496c6
Containers:
  kubeplay-arm32v7:
    Container ID:   docker://737cd78bebb982c782dae2c07f4604ba71f45bc112e91aa20652e29b25bf5d63
    Image:          jbrette/kubeplay-arm32v7:0.1.0
    Image ID:       docker-pullable://jbrette/kubeplay-arm32v7@sha256:86441999035e35514fc647ff9af90bdfc152299636b48b9d3dba9107574f3957
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 15 Jul 2018 20:39:23 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:      http-get http://:80/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-c5v7s (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-c5v7s:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-c5v7s
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned default/helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m to nas-pi
  Normal  Pulling    10m   kubelet, nas-pi    pulling image &quot;jbrette/kubeplay-arm32v7:0.1.0&quot;
  Normal  Pulled     10m   kubelet, nas-pi    Successfully pulled image &quot;jbrette/kubeplay-arm32v7:0.1.0&quot;
  Normal  Created    10m   kubelet, nas-pi    Created container
  Normal  Started    10m   kubelet, nas-pi    Started container
</code></pre><p>Let&rsquo;s open a browser towards the URL:</p>
<p><img src="/images/kubeplay/kubeplay_arm32v7.png" alt="">
<img src="/images/kubeplay/dashboard.png" alt=""></p>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>Need to cleanup the helm chart and docker image to ensure consistency of the internal</li>
<li>Need to improve the NodePort/ClusterIP/LoadBalancer handling.</li>
</ul>
<h2 id="reference-links">Reference Links</h2>
<ul>
<li><a href="https://github.com/jonaseck2/rpi-helm">Usage of remote API</a></li>
</ul>
<div class="edit-meta">
Last updated on 13 Jul 2018


<br>
Published on 13 Jul 2018
<br><a href="https://github.com/jbrette/blog/edit/master/kubernetes/rpi/2018-07-13-a.md" class="edit-page"><i class="fas fa-pen-square"></i>&nbsp;Edit on GitHub</a></div><nav class="pagination"><a class="nav nav-prev" href="https://jbrette.github.io/kubernetes/rpi/2018-07-14-a/" title="Deploy Flannel in Raspberry PI cluster"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Deploy Flannel in Raspberry PI cluster</a>
<a class="nav nav-next" href="https://jbrette.github.io/kubernetes/rpi/2018-07-12-a/" title="Enable docker remote API">Next - Enable docker remote API <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="slide-menu">
<ul>
<li class=""><a href="https://jbrette.github.io/">Home</a></li>

<li class=" has-sub-menu"><a href="https://jbrette.github.io/ai_ml/">AI &amp; ML<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://jbrette.github.io/ai_ml/2018-07-19-a/">Kubeflow</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://jbrette.github.io/iot/">IOT<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://jbrette.github.io/iot/2018-07-06-a/">ZWave/ZigBee</a></li>
<li class=""><a href="https://jbrette.github.io/iot/2018-07-05-a/">OpenHAB</a></li>
<li class=""><a href="https://jbrette.github.io/iot/2018-07-04-a/">HomeAssistant</a></li>
</ul>
  
</li>

<li class="parent has-sub-menu"><a href="https://jbrette.github.io/kubernetes/">Kubernetes<span class="mark opened">-</span></a>
  
<ul class="sub-menu">

<li class=""><a href="https://jbrette.github.io/kubernetes/operators/">Kubernetes Operators</a>
  
</li>

<li class=""><a href="https://jbrette.github.io/kubernetes/kustomize/">Kustomize</a>
  
</li>

<li class=" has-sub-menu"><a href="https://jbrette.github.io/kubernetes/devops/">DEVOPS<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://jbrette.github.io/kubernetes/devops/2018-07-11-a/">Use github repo as helm chart repository</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/devops/2018-06-25-a/">Setup github/gerrit behind a corporate http proxy</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/devops/2018-06-24-a/">docker.io versus docker-ce</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/devops/2018-06-23-a/">Setup multiple GitHub account on a single machine</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/devops/2018-06-22-a/">Setup your GOLANG environment</a></li>
</ul>
  
</li>

<li class="parent has-sub-menu"><a href="https://jbrette.github.io/kubernetes/rpi/">RASPBERRY PI<span class="mark opened">-</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-09-28-a/">Upgrade RPI Kubernetes cluster to 1.12</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-28-a/">Deploy Cassandra on Raspberry-PI 3</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-17-a/">Rebuild Calico for AMD64 ad ARM32V7</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-16-a/">Rebuild Hyperkube images</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-15-a/">Recompile Kubernetes components for Raspberry PI</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-14-a/">Deploy Flannel in Raspberry PI cluster</a></li>
<li class="active"><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-13-a/">Deploy Helm and Tiller on Rasberry PI Cluster</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-12-a/">Enable docker remote API</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-08-a/">Add Persistency Volume to PI Clusters</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-07-a/">Using Ansible to manage Raspberry PI cluster</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-07-03-a/">Creating a Raspberry 3 B&#43; Kubernetes Cluster</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-06-20-a/">Create a Rapsberry PI Rescue Dongle</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/rpi/2018-06-19-a/">Add Raspberry PI node to Kubernetes Cluster in 10 min</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://jbrette.github.io/kubernetes/sdk/">SDK<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://jbrette.github.io/kubernetes/sdk/2018-06-30-a/">Update Kubernetes to 1.11 on Ubuntu</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/sdk/2018-06-27-a/">Manual Update of CoreOS</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/sdk/2018-06-21-a/">Setup SingleNode Kubernetes Cluster using kubeadm</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://jbrette.github.io/kubernetes/apps/">APPS<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://jbrette.github.io/kubernetes/apps/2018-07-01-a/">Creating simple Python server container</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/apps/2018-06-29-a/">Creating simple GO server container</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/apps/2018-06-28-a/">Creating simple Java 10 server container</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://jbrette.github.io/kubernetes/misc/">MISC<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://jbrette.github.io/kubernetes/misc/2018-08-01-a/">Build and Deploy Kubernetes Hashicorp Vault</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/misc/2018-07-31-a/">Build and Deploy Kubernetes Istio</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/misc/2018-07-30-a/">Build and Deploy Kubernetes test-infra</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/misc/2018-07-29-a/">Build and Deploy Kubernetes Kustomize</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/misc/2018-07-18-a/">Compile and Test Portieris</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/misc/2018-07-02-a/">Compile and Test SONOBUOY</a></li>
<li class=""><a href="https://jbrette.github.io/kubernetes/misc/2018-06-26-a/">Zuul</a></li>
</ul>
  
</li>
</ul>
  
</li>










</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
